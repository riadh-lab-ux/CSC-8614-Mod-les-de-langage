{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682f35f1",
   "metadata": {},
   "source": [
    "# CSC 8614 - Language Models\n",
    "## CI3 - Parameter-Efficient Fine-Tuning with LoRA\n",
    "\n",
    "This TP builds upon the GPT architecture we have previously explored. \n",
    "\n",
    "We will implement Low-Rank Adaptation (LoRA) from scratch and inject it into our pre-existing GPTModel.\n",
    "\n",
    "Objectives:\n",
    "- Implement the mathematical formulation of LoRA.\n",
    "- Create a wrapper to convert standard Linear layers into LoRA-compatible layers.\n",
    "- Inject these layers into a pre-trained GPT model.\n",
    "- Verify that only a fraction of parameters are trainable.\n",
    "- Fine-tune the model with LoRA\n",
    "\n",
    "Some of this code comes from the book _Build a Large Language Model (From Scratch)_, by Sebastian Raschka, and its [official github repository](https://github.com/rasbt/LLMs-from-scratch).\n",
    "\n",
    "This TP will be done in this notebook, and requires some additional files (available from the course website). \n",
    "You will have to fill the missing portions of code, and perform some additional experiments by testing different parameters.\n",
    "\n",
    "Working on this TP:\n",
    "- The easiest way is probably to work directly on the notebook, using jupyter notebook or visual studio code. An alternative is also to use Google colab.\n",
    "- You should be able to run everything on your machine, but you can connect to the GPUs if needed.\n",
    "- **NOTE**: run the cells in the correct order, otherwise you might get errors due to inconsintencies.\n",
    "\n",
    "Some files are required, and are available on the course website and/or github repo:\n",
    "- `requirements.txt`\n",
    "- `gpt_utils.py`\n",
    "\n",
    "\n",
    "## About the report\n",
    "You will have to return this notebook (completed), as well as a mini-report (`TP3/rapport.md`).\n",
    "\n",
    "The notebook and report shall be submitted via a GitHub repository, similarly to what you did for the previous sessions (remember to use a different folder: `TP3`).\n",
    "For the notebook, it is sufficient to complete the code and submit the final version.\n",
    "\n",
    "For the mini-report, you have to answer the questions asked in this notebook, and discuss some of your findings as requested.\n",
    "Same as in the previous sessions:\n",
    "- You must include: short answers, observed results (copies of outputs), requested screenshots, and a brief interpretation.\n",
    "- Do not paste entire pages: be concise and select the relevant elements.\n",
    "\n",
    "Reproducibility: \n",
    "- fix a random seed and write it in the report\n",
    "- indicate in the report the specific python version OS, and the library versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8ead2",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "Install the requirements.\n",
    "\n",
    "**Note**: if you use the same virtual environment as last time, you will not have to reinstall everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347af8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.9.1\n",
      "  Using cached torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "Collecting tiktoken==0.12.0\n",
      "  Using cached tiktoken-0.12.0-cp310-cp310-win_amd64.whl (879 kB)\n",
      "Collecting tqdm==4.67.1\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting pandas==2.3.3\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Collecting matplotlib==3.10.8\n",
      "  Using cached matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "Collecting tensorflow==2.20.0\n",
      "  Using cached tensorflow-2.20.0-cp310-cp310-win_amd64.whl (331.7 MB)\n",
      "Collecting jupyterlab==4.5.1\n",
      "  Using cached jupyterlab-4.5.1-py3-none-any.whl (12.4 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (4.15.0)\n",
      "Collecting networkx>=2.5.1\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Collecting sympy>=1.13.3\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting fsspec>=0.8.5\n",
      "  Using cached fsspec-2026.1.0-py3-none-any.whl (201 kB)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2026.1.15-cp310-cp310-win_amd64.whl (277 kB)\n",
      "     ---------------------------------------- 0.0/277.8 kB ? eta -:--:--\n",
      "     ---------------------- --------------- 163.8/277.8 kB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 277.8/277.8 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting requests>=2.26.0\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from tqdm==4.67.1->-r requirements.txt (line 3)) (0.4.6)\n",
      "Collecting numpy>=1.22.4\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Collecting pyparsing>=3\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.61.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (25.0)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pillow>=8\n",
      "  Using cached pillow-12.1.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=5.28.0\n",
      "  Downloading protobuf-6.33.4-cp310-abi3-win_amd64.whl (436 kB)\n",
      "     ---------------------------------------- 0.0/437.0 kB ? eta -:--:--\n",
      "     -------------------------------------  430.1/437.0 kB 9.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 437.0/437.0 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-2.0.1-cp310-cp310-win_amd64.whl (60 kB)\n",
      "Collecting google_pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.76.0-cp310-cp310-win_amd64.whl (4.7 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting keras>=3.10.0\n",
      "  Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Collecting tensorboard~=2.20.0\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting opt_einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting h5py>=3.11.0\n",
      "  Using cached h5py-3.15.1-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.17.0)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1\n",
      "  Using cached ml_dtypes-0.5.4-cp310-cp310-win_amd64.whl (210 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (65.5.0)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Using cached flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: traitlets in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.5.4)\n",
      "Collecting jupyterlab-server<3,>=2.28.0\n",
      "  Using cached jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.9.1)\n",
      "Collecting notebook-shim>=0.2\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Using cached jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "Collecting tomli>=1.2.2\n",
      "  Using cached tomli-2.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Using cached jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Collecting httpx<1,>=0.25.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.1.0)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting idna\n",
      "  Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Collecting anyio\n",
      "  Using cached anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Collecting h11>=0.16\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (8.8.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.8.19)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.2.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (8.38.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (27.1.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.3)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from jupyter-core->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.5.1)\n",
      "Collecting jupyter-events>=0.11.0\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Collecting argon2-cffi>=21.1\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Collecting websocket-client>=1.7\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Collecting send2trash>=1.8.2\n",
      "  Downloading send2trash-2.1.0-py3-none-any.whl (17 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Collecting pywinpty>=2.0.1\n",
      "  Using cached pywinpty-3.0.2-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Collecting nbconvert>=6.4.4\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4\n",
      "  Downloading jupyter_server_terminals-0.5.4-py3-none-any.whl (13 kB)\n",
      "Collecting prometheus-client>=0.9\n",
      "  Downloading prometheus_client-0.24.1-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 0.0/64.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.1/64.1 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting overrides>=5.0\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Collecting babel>=2.10\n",
      "  Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Collecting jsonschema>=4.18.0\n",
      "  Using cached jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
      "Collecting json5>=0.9.0\n",
      "  Using cached json5-0.13.0-py3-none-any.whl (36 kB)\n",
      "Collecting optree\n",
      "  Using cached optree-0.18.0-cp310-cp310-win_amd64.whl (302 kB)\n",
      "Collecting namex\n",
      "  Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting rich\n",
      "  Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from anyio->httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.3.1)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl (31 kB)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.52)\n",
      "Requirement already satisfied: decorator in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.19.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.6.3)\n",
      "Collecting rpds-py>=0.25.0\n",
      "  Using cached rpds_py-0.30.0-cp310-cp310-win_amd64.whl (235 kB)\n",
      "Collecting attrs>=22.2.0\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting pyyaml>=5.3\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Using cached python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Collecting bleach[css]!=5.0.0\n",
      "  Using cached bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Using cached mistune-3.2.0-py3-none-any.whl (53 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Using cached nbclient-0.10.4-py3-none-any.whl (25 kB)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.8.5)\n",
      "Collecting isoduration\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting webcolors>=24.6.0\n",
      "  Using cached webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
      "Collecting jsonpointer>1.13\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0\n",
      "  Using cached rfc3987_syntax-1.1.0-py3-none-any.whl (8.0 kB)\n",
      "Collecting fqdn\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting uri-template\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.14)\n",
      "Collecting cffi>=1.0.1\n",
      "  Using cached cffi-2.0.0-cp310-cp310-win_amd64.whl (182 kB)\n",
      "Collecting soupsieve>=1.6.1\n",
      "  Using cached soupsieve-2.8.1-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\riadh\\desktop\\csc8614\\tp3\\tp3\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.1)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Collecting lark>=1.2.2\n",
      "  Using cached lark-1.3.1-py3-none-any.whl (113 kB)\n",
      "Collecting arrow>=0.15.0\n",
      "  Using cached arrow-1.4.0-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: webencodings, pytz, namex, mpmath, libclang, flatbuffers, fastjsonschema, wrapt, wheel, websocket-client, webcolors, urllib3, uri-template, tzdata, tqdm, tomli, tinycss2, termcolor, tensorboard-data-server, sympy, soupsieve, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, regex, pyyaml, pywinpty, python-json-logger, pyparsing, pycparser, protobuf, prometheus-client, pillow, pandocfilters, overrides, optree, opt_einsum, numpy, networkx, mistune, mdurl, MarkupSafe, markdown, lark, kiwisolver, jupyterlab-pygments, jsonpointer, json5, idna, h11, grpcio, google_pasta, gast, fsspec, fqdn, fonttools, filelock, defusedxml, cycler, charset_normalizer, certifi, bleach, babel, attrs, async-lru, absl-py, werkzeug, terminado, rfc3987-syntax, requests, referencing, pandas, ml_dtypes, markdown-it-py, jinja2, httpcore, h5py, contourpy, cffi, beautifulsoup4, astunparse, arrow, anyio, torch, tiktoken, tensorboard, rich, matplotlib, jupyter-server-terminals, jsonschema-specifications, isoduration, httpx, argon2-cffi-bindings, keras, jsonschema, argon2-cffi, tensorflow, nbformat, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 160, in exc_logging_wrapper\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 507, in run\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 73, in install_given_reqs\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 796, in install\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\operations\\install\\wheel.py\", line 729, in install_wheel\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\operations\\install\\wheel.py\", line 646, in _install_wheel\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 436, in make_multiple\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_internal\\operations\\install\\wheel.py\", line 427, in make\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 425, in make\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 325, in _make_script\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 249, in _write_script\n",
      "  File \"C:\\Users\\riadh\\Desktop\\CSC8614\\TP3\\tp3\\lib\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 404, in _get_launcher\n",
      "ValueError: Unable to find resource t64.exe in package pip._vendor.distlib\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979fc7",
   "metadata": {},
   "source": [
    "## Background & Mathematical Formulation\n",
    "\n",
    "Fine-tuning Large Language Models (LLMs) updates all model parameters, which is computationally expensive. LoRA freezes the pre-trained weights and injects trainable rank decomposition matrices.\n",
    "\n",
    "Given a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d_{out} \\times d_{in}}$, LoRA constrains the update $\\Delta W$ by representing it with a low-rank decomposition:\n",
    "\n",
    "$$W_0 + \\Delta W = W_0 + B A$$\n",
    "\n",
    "Where:\n",
    "- $B \\in \\mathbb{R}^{d_{out} \\times r}$\n",
    "- $A \\in \\mathbb{R}^{r \\times d_{in}}$\n",
    "- $r \\ll \\min(d_{in}, d_{out})$ is the rank.\n",
    "\n",
    "The Forward Pass:\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} (B A x)$$\n",
    "\n",
    "- $\\alpha$ is a scaling constant.\n",
    "- $A$ is initialized with random Gaussian values.\n",
    "- $B$ is initialized with zeros (so training starts with no changes to the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbec02b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and our provided GPT utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15a71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from gpt_utils import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685e3ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2120c4b8190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 123 #TODO: Set a seed for reproducibility\n",
    "torch.manual_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a222d",
   "metadata": {},
   "source": [
    "## Implementing the LoRA Layer\n",
    "\n",
    "In this exercise, you will create the LoRALayer module. This module computes the $\\Delta Wx$ term (the branch on the right side of the diagram seen during the lecture).\n",
    "\n",
    "### **Exercise 1**: Define the LoRA Module\n",
    "\n",
    "Requirements:\n",
    "1. Define dimensions for parameters A and B based on in_dim, out_dim, and rank.\n",
    "2. Initialize A with kaiming_uniform_ (or small random normal).\n",
    "3. Initialize B with zeros.\n",
    "4. Implement the forward pass including the scaling factor $\\frac{\\alpha}{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be539ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank # Scaling factor per LoRA paper\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Initialize A and B\n",
    "        # A maps from in_dim -> rank\n",
    "        # B maps from rank -> out_dim\n",
    "        # Hint: use nn.Parameter\n",
    "        # ---------------------------------------------------------\n",
    "        self.A = nn.Parameter(torch.empty(rank, in_dim))\n",
    "        self.B = nn.Parameter(torch.empty(out_dim, rank))\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Apply Initializations\n",
    "        # A: Kaiming Uniform (a=sqrt(5))\n",
    "        # B: Zeros\n",
    "        # Init\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Calculate the LoRA output\n",
    "        # ---------------------------------------------------------\n",
    "        result = ((x @ self.A.t()) @ self.B.t()) * self.scaling\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767c887",
   "metadata": {},
   "source": [
    "## Wrapping Linear Layers\n",
    "\n",
    "We rarely replace the layer entirely; instead, we want a layer that holds both the frozen original weights and the new LoRA weights.\n",
    "\n",
    "### **Exercise 2**: The LinearWithLoRA Wrapper\n",
    "\n",
    "Requirements:\n",
    "1. Store the original linear layer.\n",
    "2. Create a self.lora instance using the dimensions of the original linear layer.\n",
    "3. In forward, add the output of the original layer to the output of the LoRA layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d268ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed: Wrapper acts identically to original layer at initialization.\n"
     ]
    }
   ],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        \n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Instantiate the LoRALayer\n",
    "        # Use linear_layer.in_features and linear_layer.out_features\n",
    "        # ---------------------------------------------------------\n",
    "        self.lora = LoRALayer(\n",
    "            in_dim=linear_layer.in_features,\n",
    "            out_dim=linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------------------------------------------------\n",
    "        # TODO: Combine Frozen + LoRA paths\n",
    "        # ---------------------------------------------------------\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# --- Sanity Check ---\n",
    "# Create a dummy layer to test\n",
    "input_x = torch.randn(1, 128)\n",
    "original_layer = nn.Linear(128, 64)\n",
    "lora_wrapped = LinearWithLoRA(original_layer, rank=4, alpha=8)\n",
    "\n",
    "# Because B is initialized to zeros, the outputs should be identical initially\n",
    "assert torch.allclose(original_layer(input_x), lora_wrapped(input_x))\n",
    "print(\"Test Passed: Wrapper acts identically to original layer at initialization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201146a1",
   "metadata": {},
   "source": [
    "## Injecting LoRA into GPTModel\n",
    "\n",
    "Now we need to modify our existing GPTModel. We cannot manually rewrite the class. Instead, we will iterate through the model's modules and replace specific layers dynamically.\n",
    "\n",
    "In GPTModel, the transformer blocks are stored in *self.trf_blocks*. Inside those, we have attention mechanisms (att) containing W_query, W_key, W_value, or a combined c_attn.\n",
    "\n",
    "Note: For this lab, to keep it simple, we will replace all nn.Linear layers except the final output head.\n",
    "\n",
    "### **Exercise 3**: Recursive Model Modification\n",
    "\n",
    "Requirements:\n",
    "1. Iterate through named children of the model.\n",
    "2. If a module is nn.Linear, wrap it in LinearWithLoRA.\n",
    "3. Important: Skip the final output layer (often named out_head or similar in gpt_utils), as we usually don't want to reduce the rank of the vocabulary projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116d7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    \"\"\"\n",
    "    Recursively replaces nn.Linear with LinearWithLoRA.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Skip final vocabulary projection\n",
    "            if name == \"out_head\":\n",
    "                continue\n",
    "\n",
    "            new_layer = LinearWithLoRA(module, rank=rank, alpha=alpha)\n",
    "            setattr(model, name, new_layer)\n",
    "\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5f1d3",
   "metadata": {},
   "source": [
    "## Freeze & Verify\n",
    "\n",
    "We have injected the layers, but currently, everything is still trainable. We must freeze the original weights.\n",
    "\n",
    "### **Exercise 4**: Freezing and Counting Parameters\n",
    "Requirements:\n",
    "1. Set requires_grad = False for all parameters.\n",
    "2. Iterate through the model; if a layer is LinearWithLoRA, unfreeze self.lora.A and self.lora.B.\n",
    "3. Calculate the ratio of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5764f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_and_activate_lora(model):\n",
    "    # 1) Freeze ALL parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2) Unfreeze only LoRA A and B matrices\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.lora.A.requires_grad = True\n",
    "            module.lora.B.requires_grad = True\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    \n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            \n",
    "    print(f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab6916",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's download and initialize the model from gpt_utils, then apply our LoRA transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73aae7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 26.2kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 955kiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 45.4kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [04:06<00:00, 2.02MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 2.67MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 518kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 597kiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights downloaded and loaded into memory.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "# (Basically same code as last session)\n",
    "\n",
    "from gpt_utils import GPTModel, download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "# Download the model weights (124M param version), and initialize it.\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2_weights\")\n",
    "print(\"Weights downloaded and loaded into memory.\")\n",
    "\n",
    "# Configure the model, mapping OpenAI specific keys to our model's keys (if needed)\n",
    "model_config = {\n",
    "    \"vocab_size\": settings[\"n_vocab\"],\n",
    "    \"context_length\": settings[\"n_ctx\"],\n",
    "    \"emb_dim\": settings[\"n_embd\"],\n",
    "    \"n_heads\": settings[\"n_head\"],\n",
    "    \"n_layers\": settings[\"n_layer\"],\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True,\n",
    "}\n",
    "\n",
    "# Initialize the Base Model\n",
    "model = GPTModel(model_config)\n",
    "load_weights_into_gpt(model, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26ffba",
   "metadata": {},
   "source": [
    "Now, we call all the methods we have defined above, and put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4001e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Structure (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Model Structure After LoRA (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_key): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_value): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (out_proj): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (1): GELU()\n",
      "      (2): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Parameter Count:\n",
      "trainable params: 1,327,104 || all params: 164,364,288 || trainable%: 0.81%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "print(\"Original Model Structure (Truncated):\")\n",
    "print(model.trf_blocks[0]) # Print first block to see standard Linear layers\n",
    "\n",
    "# Apply LoRA Replacement\n",
    "# Rank 8, Alpha 16 (Alpha is usually set to 2x Rank as a rule of thumb)\n",
    "replace_linear_with_lora(model, rank=8, alpha=16)\n",
    "\n",
    "# 3. Freeze Weights\n",
    "freeze_and_activate_lora(model)\n",
    "\n",
    "# 4. Check Results\n",
    "print(\"\\nModel Structure After LoRA (Truncated):\")\n",
    "print(model.trf_blocks[0])\n",
    "\n",
    "print(\"\\nParameter Count:\")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a264553",
   "metadata": {},
   "source": [
    "**Question 1:** Do you see any difference between \"Original Model Structure (Truncated)\" and \"Model Structure After LoRA (Truncated)\"? Do you see the LinearWithLoRA you have defined above?\n",
    "\n",
    "**Question 2:** What is the number of trainable parameters, all parameters, and the fraction of trainable parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e97c6",
   "metadata": {},
   "source": [
    "## Training Loop Verification\n",
    "\n",
    "Finally, let's prove that gradients are only generated for the specific LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8076639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Verification:\n",
      "Parameters with gradients: 144\n",
      "Frozen parameters correctly without gradients: 197\n",
      "SUCCESS: Gradients are flowing correctly only into LoRA parameters.\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 2\n",
    "dummy_input = torch.randint(0, 1000, (batch_size, 256))\n",
    "dummy_target = torch.randint(0, 1000, (batch_size, 256))\n",
    "\n",
    "# Optimizer setup\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-3)\n",
    "\n",
    "# Forward Pass\n",
    "logits = model(dummy_input)\n",
    "loss = torch.nn.functional.cross_entropy(logits.view(-1, model_config['vocab_size']), dummy_target.view(-1))\n",
    "\n",
    "# Backward Pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Verification Step\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nGradient Verification:\")\n",
    "grads_found = 0\n",
    "grads_missing = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param.grad is not None:\n",
    "            grads_found += 1\n",
    "        else:\n",
    "            print(f\"WARNING: Trainable parameter {name} has no gradient.\")\n",
    "    else:\n",
    "        if param.grad is not None:\n",
    "            print(f\"ERROR: Frozen parameter {name} has a gradient!\")\n",
    "        else:\n",
    "            grads_missing += 1\n",
    "\n",
    "print(f\"Parameters with gradients: {grads_found}\")\n",
    "print(f\"Frozen parameters correctly without gradients: {grads_missing}\")\n",
    "\n",
    "if grads_found > 0 and grads_missing > 0:\n",
    "    print(\"SUCCESS: Gradients are flowing correctly only into LoRA parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff671a8",
   "metadata": {},
   "source": [
    "## SPAM Classification\n",
    "\n",
    "Let's now work the Spam Classification task again, but this time using the LoRA-adapted model. \n",
    "This follows the example from the previous session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933e138",
   "metadata": {},
   "source": [
    "### Download and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8beff640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset downloaded and extracted.\n",
      "Loaded 5572 examples.\n",
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Balanced Dataset size: 1494\n",
      "Size of train_df: 1195\n",
      "Size of test_df: 299\n",
      "Distribution of labels in the dataset:\n",
      "train:\n",
      "label\n",
      "1    598\n",
      "0    597\n",
      "Name: count, dtype: int64\n",
      "test:\n",
      "label\n",
      "0    150\n",
      "1    149\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = os.path.join(extracted_path, \"SMSSpamCollection\")\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    print(\"Dataset downloaded and extracted.\")\n",
    "\n",
    "# Load into DataFrame\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "print(f\"Loaded {len(df)} examples.\")\n",
    "print(df.head())\n",
    "\n",
    "# Class Balancing (primarily for speed in the lab)\n",
    "spam_df = df[df[\"label\"] == \"spam\"]\n",
    "ham_df = df[df[\"label\"] == \"ham\"].sample(len(spam_df), random_state=RANDOM_STATE)\n",
    "df = pd.concat([spam_df, ham_df]).reset_index(drop=True)\n",
    "\n",
    "# Map labels to integers\n",
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "print(f\"Balanced Dataset size: {len(df)}\")\n",
    "\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "print(f\"Size of train_df: {len(train_df)}\")\n",
    "print(f\"Size of test_df: {len(test_df)}\")\n",
    "\n",
    "print(\"Distribution of labels in the dataset:\")\n",
    "print(\"train:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"test:\")\n",
    "print(test_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb81f48",
   "metadata": {},
   "source": [
    "Define Dataset and DataLoader, similarly to previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b33971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "        \n",
    "        # Truncate\n",
    "        encoded = encoded[:self.max_length]\n",
    "        \n",
    "        # Pad (GPT-2 usually uses <|endoftext|> as padding)\n",
    "        pad_len = self.max_length - len(encoded)\n",
    "        encoded = encoded + [50256] * pad_len # 50256 is <|endoftext|> in GPT2\n",
    "        \n",
    "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Create Loaders\n",
    "train_loader = DataLoader(SpamDataset(train_df, tokenizer), batch_size=8, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(SpamDataset(test_df, tokenizer), batch_size=8, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b9dc8",
   "metadata": {},
   "source": [
    "We now modify the Model for Classification, replacing the final layer (out_head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e686056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Output Head: Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# Check input dimension of the current head\n",
    "hidden_dim = model.out_head.in_features\n",
    "\n",
    "# Replace the head\n",
    "model.out_head = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "# Move model to device (if using GPU, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"New Output Head:\", model.out_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4b703",
   "metadata": {},
   "source": [
    "We previously froze everything except LoRA. Now we added a new head, let's make it unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6889edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,328,642 || all params: 125,768,450 || trainable%: 1.06%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def set_classification_trainable(model):\n",
    "    # Ensure LoRA layers are trainable (A and B)\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.lora.A.requires_grad = True\n",
    "            module.lora.B.requires_grad = True\n",
    "    \n",
    "    # Ensure the new classification head is trainable\n",
    "    for param in model.out_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "set_classification_trainable(model)\n",
    "\n",
    "# Verify count again\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dec47a",
   "metadata": {},
   "source": [
    "**Question 3:** Check the number (and fraction) of trainable parameters, and compare it with the one above. Do you see any differences? Can you describe them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5f0ac",
   "metadata": {},
   "source": [
    "The Training Loop\n",
    "Context: Standard PyTorch loop. Note that GPT models output [batch, seq_len, hidden]. For classification, we usually take the hidden state of the last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03d3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import time\n",
    "\n",
    "def train_classifier(model, loader, optimizer, device, epochs=1):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            # The model outputs (batch, seq_len, num_classes)\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Select the last token for classification\n",
    "            # last_token_logits = logits[:, -1, :]  \n",
    "            # NOTE: this (the line above) was an error in the code provided with the previous lab: it was using the last token (which is most often PAD), \n",
    "            #   not the last non padding token. \n",
    "            # Select the last NON-PADDING token\n",
    "            #   Create a mask (1 for real tokens, 0 for PAD); 50256 is the PAD token ID\n",
    "            mask = (inputs != 50256)\n",
    "            \n",
    "            # Find the index of the last real token\n",
    "            #    Summing the mask gives the length. Subtract 1 for 0-based index.\n",
    "            #    .clamp(min=0) prevents errors if a sequence is empty (unlikely)\n",
    "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
    "            \n",
    "            # Select the logits at those specific indices\n",
    "            #    We use torch.arange for the batch dimension\n",
    "            batch_indices = torch.arange(inputs.size(0), device=device)\n",
    "            last_token_logits = logits[batch_indices, last_idx, :]\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(last_token_logits, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy for monitoring\n",
    "            preds = torch.argmax(last_token_logits, dim=-1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        acc = correct / total * 100\n",
    "        print(f\"Epoch {epoch+1} Finished | Avg Loss: {total_loss/len(loader):.4f} | Acc: {acc:.2f}% | Time: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d347648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 0 | Loss: 5.6187\n",
      "Epoch 1 | Batch 10 | Loss: 3.3566\n",
      "Epoch 1 | Batch 20 | Loss: 4.6393\n",
      "Epoch 1 | Batch 30 | Loss: 2.2519\n",
      "Epoch 1 | Batch 40 | Loss: 0.4893\n",
      "Epoch 1 | Batch 50 | Loss: 0.1079\n",
      "Epoch 1 | Batch 60 | Loss: 0.4809\n",
      "Epoch 1 | Batch 70 | Loss: 1.0254\n",
      "Epoch 1 | Batch 80 | Loss: 0.1420\n",
      "Epoch 1 | Batch 90 | Loss: 0.4288\n",
      "Epoch 1 | Batch 100 | Loss: 0.0573\n",
      "Epoch 1 | Batch 110 | Loss: 0.0314\n",
      "Epoch 1 | Batch 120 | Loss: 0.8183\n",
      "Epoch 1 | Batch 130 | Loss: 0.0018\n",
      "Epoch 1 | Batch 140 | Loss: 0.0258\n",
      "Epoch 1 Finished | Avg Loss: 1.4580 | Acc: 80.45% | Time: 3073.21s\n",
      "Epoch 2 | Batch 0 | Loss: 0.0635\n",
      "Epoch 2 | Batch 10 | Loss: 0.0018\n",
      "Epoch 2 | Batch 20 | Loss: 0.0333\n",
      "Epoch 2 | Batch 30 | Loss: 0.1305\n",
      "Epoch 2 | Batch 40 | Loss: 0.2540\n",
      "Epoch 2 | Batch 50 | Loss: 0.3653\n",
      "Epoch 2 | Batch 60 | Loss: 0.1972\n",
      "Epoch 2 | Batch 70 | Loss: 0.0242\n",
      "Epoch 2 | Batch 80 | Loss: 0.0117\n",
      "Epoch 2 | Batch 90 | Loss: 0.0563\n",
      "Epoch 2 | Batch 100 | Loss: 0.0020\n",
      "Epoch 2 | Batch 110 | Loss: 0.0015\n",
      "Epoch 2 | Batch 120 | Loss: 0.0035\n",
      "Epoch 2 | Batch 130 | Loss: 0.0005\n",
      "Epoch 2 | Batch 140 | Loss: 0.9141\n",
      "Epoch 2 Finished | Avg Loss: 0.1195 | Acc: 96.39% | Time: 988.60s\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Setup Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad], \n",
    "    lr=1e-4    # TODO: Potentially test with different learning rates\n",
    ")\n",
    "\n",
    "# Run Training\n",
    "train_classifier(model, train_loader, optimizer, device, epochs=2)  # TODO: Potentially test with different numbers of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02917255",
   "metadata": {},
   "source": [
    "**Question 4:** Can you describe the trend of the loss, and the final accuracy. Is it reasonable considering the task at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e655eb",
   "metadata": {},
   "source": [
    "We can now test the accuracy on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd866e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def evaluate_accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Select last \"real\" token logits\n",
    "            mask = (inputs != 50256)\n",
    "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
    "            batch_idx = torch.arange(inputs.size(0), device=device)\n",
    "            last_token_logits = logits[batch_idx, last_idx, :]\n",
    "            # Select last token logits (same logic as training)\n",
    "            # last_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            # Compare with targets\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "553c3a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 97.32%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Run evaluation\n",
    "test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
    "print(f\"Test Set Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713ca7",
   "metadata": {},
   "source": [
    "**Question 5:** How is the accuracy, and how does it compare to the Train set accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1b259",
   "metadata": {},
   "source": [
    "Finally, we can do a quick inference test, to see how the model classifies new texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f06b8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def classify_text(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded = encoded[:256] # Truncate\n",
    "    tensor_input = torch.tensor([encoded], dtype=torch.long).to(device) # Add batch dim\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_input)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).item()\n",
    "        \n",
    "    label_map = {0: \"HAM (Normal)\", 1: \"SPAM\"}\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Prediction: {label_map[pred_label]} (Confidence: {probs[0][pred_label]:.2f})\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d358eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'There is a big cash prize for you, call immediately.'\n",
      "Prediction: HAM (Normal) (Confidence: 0.99)\n",
      "------------------------------\n",
      "Text: 'Hey, are we still meeting for lunch tomorrow?'\n",
      "Prediction: HAM (Normal) (Confidence: 1.00)\n",
      "------------------------------\n",
      "Text: 'URGENT! You won a free ticket. Reply WIN now!'\n",
      "Prediction: SPAM (Confidence: 0.87)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: add the text you want to test.\n",
    "classify_text(\"There is a big cash prize for you, call immediately.\", model, tokenizer, device)\n",
    "classify_text(\"Hey, are we still meeting for lunch tomorrow?\", model, tokenizer, device)\n",
    "classify_text(\"URGENT! You won a free ticket. Reply WIN now!\", model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4d416",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
